[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog posts here are a mix of technical and non-technical topics, mostly things that I think about. They do not have a set pattern and not all of them would be very thorough. But enjoy anyway ;) !\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nWords\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\nNeurIPS 2025 Takeaways\n\n\n~1000\n\n\n\n\n\n\nOct 7, 2024\n\n\nLooking inside LLMs\n\n\n~300\n\n\n\n\n\n\nFeb 5, 2024\n\n\nTraining DNNs on Microcontrollers\n\n\n~1500\n\n\n\n\n\n\nSep 10, 2023\n\n\nResponsibility Ltd.\n\n\n~1200\n\n\n\n\n\n\nJul 31, 2023\n\n\nHiring and Managing Data Scientists\n\n\n~1500\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/edgeAI.html",
    "href": "portfolio/edgeAI.html",
    "title": "Efficient/Edge AI ",
    "section": "",
    "text": "Summary\nDesigning the whole pipeline ‚Äì edge based neural network architecture, pruning and quantization techniques and automatic embedded code generation.\n\n\nThe story\nThe work in this project was done mainly under a technology demonstrator (or proof of concept) for CeADAR - Ireland‚Äôs Center for Applied AI, which can be checked here. The whole pipeline starting from creating a deep neural network with low memory and computational footprint to pruning and quantizing that network for deployment and finally automatically generating embedded code to deploy the network on an embedded device running ARM processor was created. The diagram below shows this whole pipeline.\n\n\n\n\n\nFig. 1: Overview of complete Edge AI pipeline\n\n\n\n\nCollaborating with Dr.¬†Deepu John, the problem we tackled was anomaly detection in heartbeats and proposed a novel architecture inspired from auto-encoders. Adding additional neurons for feature augmentation to auto-encoders and chopping off the decoded layer we got a classifier with very low memory and computational footprint, making it easier to deploy on edge. The diagram below shows the architecture.\n\n\n\n\n\nFig. 2: Architecture of deep neural network deployed at the edge\n\n\n\n\nWe also created a novel pruning technique based on the ability of neurons to separate classes. This technique was patented and published and the diagram below shows the metric we proposed class-separation score (css). For more details check the paper here.\n\n\n\n\n\nFig. 3: Class-separation-score, a metric we designed to prune neural networks\n\n\n\n\nAlso, a collaboration with UCD Laboratory for Advanced Manufacturing Simulation and Robotics led to the paper, ‚ÄúReview and application of Edge AI solutions for mobile collaborative robotic platforms‚Äù, in Procedia CIRP, which can be found here.\nThis is still an exciting field with lots of new inventions coming frequently, like more specialized hardware and Neural Architecture Search (NAS) for the edge.\n\n\n\nImportant links\n\nPublications\n\nI. Preet, O. Boydell and D. John, ‚ÄúClass-Separation Preserving Pruning for Deep Neural Networks,‚Äù in IEEE Transactions on Artificial Intelligence, doi: 10.1109/TAI.2022.3228511, link.\nAswin K Ramasubramanian, Robins Mathew, Inder Preet, Nikolaos Papakostas, ‚ÄúReview and application of Edge AI solutions for mobile collaborative robotic platforms‚Äù, Procedia CIRP, Volume 107, 2022, link.\n\n\n\nPatent\n‚ÄòClass Separation Aware Artificial Neural Network Pruning Method‚Äô\nInternational (PCT) Application No.¬†PCT/EP2022/085998\n\n\nVideos\n\nCeADAR Demonstrator: Edge AI\nHardware for Machine Learning, an overview\nPruning and Quantization for DNNs - PyCon Ireland\n\n\n\nLink to CeADAR‚Äôs Edge AI demonstrator\nhttps://ceadar.ie/blog/edge-ai/"
  },
  {
    "objectID": "portfolio/ppml.html",
    "href": "portfolio/ppml.html",
    "title": "Privacy Preserving ML",
    "section": "",
    "text": "Summary\nPedagogic activities for companies wanting to adopt Privacy Preserving Machine Learning (PPML).\n\n\nThe story\nPrivacy is increasingly becoming a concern with vast amounts of data and advanced ML/AI algorithms. There are several ways in which ML can help preserve privacy. In this project at CeADAR, we created a series of short videos covering the different ways like anonymization, synthetic data generation and differential privacy which can be used to make data more private. We also created a short course on PPML accessible to CeADAR‚Äôs members. Below we list the pedagogic videos we created during the project.\n\nTechnical discussion with a few industry experts working on PPML \n\n\n\n\nOverview of PPML\n\n\n\nSome use cases for PPML \nSynthetic Data Generation for privacy protection \nDifferential Privacy: Part 1 \n\n\n\nDoes anonymization alone ensure privacy? \nUsing Synthetic Data Vault library \nDifferential Privacy: Part 2"
  },
  {
    "objectID": "portfolio/genAI.html",
    "href": "portfolio/genAI.html",
    "title": "Generative AI ",
    "section": "",
    "text": "Summary\nI have worked on a few projects which involved generative models like GANs and LLMs.\n\n\nUnderstanding LLMs\nI recent wrote an small article on parameter distribution of LLMs and app to play around with some open-source LLMs. You can check the blog here.\n\n\nGANs for satellite image segementation\nI worked on the problem of image segmentation in satellite images. Figure below shows a satellite image on the left with segmentation map consisting of roads, water, buildings and forest.\n\n\n\n\n\nFig. 2: Satellite image on the left with segmentation map showing roads, water, buildings and forest on the right.\n\n\n\n\nA common problem with segmentation in satellite images is that often the ground truth is not completely labelled, e.g., some roads might be left out of the labels in training data. Training a deep neural network becomes difficult as we do not have the complete ground truth, as such we need to design the loss function such that if we are confident enough in some prediction we incorporate that in ground truth. To do that I used GANs (Generative Adversarial Networks) and borrow ideas from two different studies, UNet and one by Mittal et. al. Additional terms to the loss function were added and some modifications to the latter‚Äôs pipeline were made. These modifications yielded better segmentation accuracy for some classes in the dataset.\n\n\n\n\n\nFig. 3: Modifications made to GAN: Added additional loss terms, removed a branch. Image source from Mittal et. al."
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media & Publications",
    "section": "",
    "text": "This page lists various publications, patents and presentations I have done over the years.\n\nPublications\n\nI. Preet, O. Boydell and D. John, ‚ÄúClass-Separation Preserving Pruning for Deep Neural Networks,‚Äù in IEEE Transactions on Artificial Intelligence, doi: 10.1109/TAI.2022.3228511, link.\nAswin K Ramasubramanian, Robins Mathew, Inder Preet, Nikolaos Papakostas, ‚ÄúReview and application of Edge AI solutions for mobile collaborative robotic platforms‚Äù, Procedia CIRP, Volume 107, 2022, link.\nAlastair McKinstry, Oisin Boydell, Quan Le, Inder Preet, Jennifer Hanafin, Manuel Fernandez, Adam Warde, Venkatesh Kannan, and Patrick Griffiths, ‚ÄúAI-Ready Training Datasets for Earth Observation: Enabling FAIR data principles for EO training data‚Äù. EGU General Assembly, 2021, link.\n\n\n\nPatents\n\nClass Separation Aware Artificial Neural Network Pruning Method, International (PCT) Application No.¬†PCT/EP2022/085998\nSystem and method for optimized PV placement and battery energy storage capacity.System and method for optimized PV placement and battery energy storage capacity. P23-1731WO01\n\n\n\nVideos\n\n\n\nPyCon Ireland, 2022\n\nPruning & Quantization for Deep Neural Networks&gt; Invited for a talk on my publication on optimizing DNNs for deployment at the edge.\n\n\n\n\nPodcast with experts on Privacy Preserving ML\n\nHear experts from IBM, Inpher and Oblivious.AI for a discussion on three technologies that can be used to protect data and models - differential privacy, secret computing and secure enclaves.\n\n\n\n\nHardware for Machine Learning\n\nMain challenges for deploying and training ML algorithms is discussed here and some solutions both on the hardware side and design of neural networks are presented.\n\n\n\n\nAutomatically optimizing DNNs for the edge\n\nA proof of concept for automating the optimization, pruning and quantization of DNNs for edge deployment is presented for the problem of anomaly detection in heartbeats using wearable sensors."
  },
  {
    "objectID": "blog/neurips25.html",
    "href": "blog/neurips25.html",
    "title": "NeurIPS 2025 Takeaways",
    "section": "",
    "text": "It was my first time attending NeurIPS, and it was quite something‚Äîto see so many people whose papers you‚Äôve read and whose work you‚Äôve admired, all together under the same roof. It genuinely felt like being in the driver‚Äôs seat of the AI buzz we‚Äôre going through. So, I have put together a few takeaways from the conference that I would like to share."
  },
  {
    "objectID": "blog/neurips25.html#benchmarks-for-agents-and-beyond",
    "href": "blog/neurips25.html#benchmarks-for-agents-and-beyond",
    "title": "NeurIPS 2025 Takeaways",
    "section": "Benchmarks for agents and beyond",
    "text": "Benchmarks for agents and beyond\nThe very first day was quite interesting with a lot of contrasting ideas. The very first talk was from Apple, checkout their paper The Illusion of Thinking for more background, but the crux of it was how reasoning models performed beyond benchmarks. In their study, they showed how reasoning models behave at games like Towers of Hanoi, Checkers jumping, etc. And discovered something counter-intuitive: at low-complexity games, non-reasoning models perform the same as reasoning models; at medium complexity, reasoning models are better; but at high complexity, both are bad, despite increasing the thinking budget. And, quite interestingly, the number of thinking tokens also does not increase indefinitely; it plateaus once complexity is high enough. The point the sudy made is that performance of reasoning models which are considered to be state-of-the-art for LLMs is severley constrained.\nBut the same day there was an extensive tutorial on benchmarks, including a very interesting panel discussion with panelists from public-policy, Metr, creators of SWE Bench the coding benchmark, and a few more. It was an extensive disucssion but the following two have stuck with me:\n\nIt is tricky to talk about creating a benchmark. Some have stood the test of time‚Äîor, as someone said, are being used far beyond they should be.\nThe panelist from Metr also expressed some displeasure about ‚Äòexotic‚Äô benchmarks like ARC-AGI, which he claimed (I‚Äôm paraphrasing) do not correspond to real-world applications and are instead someone‚Äôs idea of what intelligence could be‚Äîspatial reasoning in this case. I have often, been annoyed preparing for these exams that give you riddles testing what they call spatio-temporal reasoning. Is that real intelligence? Is that even a good proxy?\n\nThat inevitably made me think of the Apple talk in the morning, which now also seemed like an ‚Äòexotic‚Äô game chosen as a proxy for intelligence. The authors were aware of this and have responded to critics, but the justification felt less convincing when viewed from this angle. The strategy seems to be: choose a game the LLMs have likely never seen during training and then extrapolate performance there to draw conclusions about general reasoning capabilities. But yes, I do understand that intelligence itself is an ill-defined concept and this evaluation problem will always be slippery. Nevertheless, these studies and ‚Äòexotic datasets‚Äô are interesting because they shed light on new domains for LLMs and who knows where the next breakthrough comes from.\nApart from these highly philosophical discussions, there were some very practical trends around benchmarking agents. We are all aware of the limitations of traditional metrics, the overly homogeneous test sets, data drift, and so on. But a new, dynamic testing framework is emerging with agents: generating new test instances during evaluation, keeping the test set alive instead of static. The key metric becomes whether the agent accomplishes the task, however you want to quantify that, instead of the accuracy of components like word-error rate or F1 score. A clear shift from what I‚Äôd call academic metrics to more product-centric ones‚Äînobody should be unhappy about that. One of the best examples came at the end of the day from Tesla‚Äôs driving agent. They built a world model from the millions of miles of driving data they have, and they can edit the inputs to this world model using natural language to create new scenarios on the road to test their self-driving agent. A closed-loop simulator for testing and training agents. Honestly the best production-grade example I saw.\nBeeAI framework from IBM also fits neatly into this new paradigm of dynamic evaluation, though I‚Äôll write more on that separately."
  },
  {
    "objectID": "blog/neurips25.html#coding-agents",
    "href": "blog/neurips25.html#coding-agents",
    "title": "NeurIPS 2025 Takeaways",
    "section": "Coding agents",
    "text": "Coding agents\nResearch and industry both recognize the limitations of current coding agents when it comes to production-quality code. Everyone acknowledges the issues with SWE-bench, and they admit that getting real production data for coding benchmarks is still a major bottleneck. If that‚Äôs the bottleneck, then are we just one step away from Microsoft (owning GitHub) or Google‚Äîwith their armies of engineers locking down the advantage? Maybe. But Claude Code doing extremely well keeps me hopeful that we‚Äôll see meaningful competition rather than consolidation.\nI see a new trend emerging as well, a huge proportion of papers on coding agents revolve around clever prompting strategies and decomposing problems into flows that might help the agent. But isn‚Äôt that just what all of us do anyway? You sketch a high-level plan before solving anything. It reminds me of a few years ago when ML use exploded and thousands of papers emerged where people applied standard ML techniques to a new domain, and master‚Äôs theses consisted of doing a giant grid search. Mine was the same, by the way ;)\nA cute piece of work was Paper2Code which talks about the forever needed task of generating code from research papers to replicate their studies or build them further. Something we‚Äôve all desperately needed, so kudos to them.\nIBM‚Äôs ALICE - Agentic Logic for Incident and Code bug Elimination, is another one I‚Äôm proud to see progress. It brings together multi-agent collaboration, incident analysis, and code-level reasoning to help teams diagnose complex system issues faster and more intelligently. This sits right at the intersection of AI agents, observability, and software automation‚Äîan area that‚Äôs evolving ridiculously fast."
  },
  {
    "objectID": "blog/neurips25.html#efficient-ai",
    "href": "blog/neurips25.html#efficient-ai",
    "title": "NeurIPS 2025 Takeaways",
    "section": "Efficient AI",
    "text": "Efficient AI\nThere was a lot happening on Efficient AI. The most intriguing work was from companies like Eigen AI, Pruna AI, Furiosa AI, and Qualcomm (check out their demo on disaggregated LLM serving on AI accelerators). A lot of practical solutions for energy saving, compute optimization are finally making their way into the market with an ever crunching demand for chips.\nAs usual Han Lab stood out with: Jet-Nemotron and radial attention‚Äîa sparse attention mechanism for long video generation. Sparse models are clearly becoming the theme again, but this time in they are expanding into other modalities as well."
  },
  {
    "objectID": "blog/neurips25.html#wrapping-up",
    "href": "blog/neurips25.html#wrapping-up",
    "title": "NeurIPS 2025 Takeaways",
    "section": "Wrapping up",
    "text": "Wrapping up\nAs always, a lot of the best parts of NeurIPS happen outside the sessions. The side conversations were rich, people comparing notes on agent evaluations, debugging LLM behaviors in the wild, trying to make sense of the rapid shifts in hardware, and the shared feeling that we‚Äôre building tools faster than we can define what ‚Äúgood‚Äù looks like. It‚Äôs chaotic, but in the fun way.\nAnd yes, I left with more questions than answers, but that‚Äôs the whole point of going."
  },
  {
    "objectID": "blog/llms.html",
    "href": "blog/llms.html",
    "title": "Looking inside LLMs",
    "section": "",
    "text": "Meta launched its Llama 3.1 model not too long ago claiming it to be the biggest open source LLM so far. And as such there has been a flurry of open-source LLMs out in the market. Companies are spending thousands in training these models and millions in the infrastructure needed to train and deploy these models.\nFrom a personal perspective, I am fascinated by the architecture of these vast networks‚Äîthe layers, the weight distributions, and more. In my previous work 1, we employed the activations of different layers to prune the neural network, eliminating redundant neurons and thereby shrinking the model for edge deployment. A key insight from that study was that the internal representations of these networks‚Äîoften sparse despite having billions of parameters‚Äîcontain valuable information that can further our understanding of the models. Interestingly, a significant number of these parameters are zero or near-zero, underscoring the inherent sparsity of these networks. Yet, this has not deterred corporate investments in discovering these ‚Äúgiganormous‚Äù sparse matrices. Over time, we anticipate learning more efficient methods to uncover these structures, which may also illuminate aspects of LLM explainability, as several studies have attempted by analyzing network activations.\nAs an illustrative exercise, I have plotted Kernel Density Estimates (KDEs)‚Äîan advanced technique for visualizing distributions, superior to traditional histograms (for newcomers to KDE, see here 2). These plots reveal various patterns in the parameters of several open-source LLMs. I encourage you to explore these graphs and share your observations in the comments below.\nIn the coming days, I plan to dive deeper into the exploration of these open-source LLMs and learn more about their underlying parameters. Stay tuned for more insights."
  },
  {
    "objectID": "blog/llms.html#kernel-density-estimates-for-llm-parameters",
    "href": "blog/llms.html#kernel-density-estimates-for-llm-parameters",
    "title": "Looking inside LLMs",
    "section": "Kernel Density Estimates for LLM parameters",
    "text": "Kernel Density Estimates for LLM parameters\n\n\n\n\n\n\nLlama 3.1 405B - Instruct\n\n\n\n\n\n\n\n\n\n\n\nattention.wk.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wv.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wq.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wo.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention_norm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w1.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w2.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w3.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nffn_norm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMistral 7B\n\n\n\n\n\n\n\n\n\n\n\nattention.wk.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wv.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wq.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention.wo.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattention_norm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w1.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w2.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeed_forward.w3.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nffn_norm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT 2-medium 355B\n\n\n\n\n\n\n\n\n\n\n\nattn.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattn.c_attn.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattn.c_attn.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattn.c_proj.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nattn.c_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nln_1.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nln_1.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nln_2.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nln_2.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.c_fc.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.c_fc.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.c_proj.bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.c_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGemma 7B - Instruct\n\n\n\n\n\n\n\n\n\n\n\ninput_layernorm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.down_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.gate_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlp.up_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_attention_layernorm.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nself_attn.o_proj.weight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nself_attn.qkv_proj.weight"
  },
  {
    "objectID": "blog/llms.html#footnotes",
    "href": "blog/llms.html#footnotes",
    "title": "Looking inside LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClass-Separation Preserving Pruning for Deep Neural Networks‚Ü©Ô∏é\nKernel Density Estimates‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a Lead AI Scientist in the Speech team at IBM. My goal is to create profitable products from cutting edge AI often leading to new forntiers in research as well.\n Experience : Areas that I have worked in include- speech models (STT-TTS), Efficient AI, Foundation models and time-series analysis. Checkout my portfolio for more information on all these. \n Educational Background  : I graduated from St.¬†Stephen‚Äôs College, Delhi and did a masters in Computational science from University College Dublin, where my thesis was in machine learning.\n Companies I‚Äôve Worked For \n\n    \n        \n    \n    \n        \n    \n    \n        \n    \n    \n        \n    \n\n Contact : me[at]inderpreet[dot]ie"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Services offered - One on one consultancy - Research help"
  },
  {
    "objectID": "blog/responsibility_ltd.html",
    "href": "blog/responsibility_ltd.html",
    "title": "Responsibility Ltd.",
    "section": "",
    "text": "Observe what you say for about two or three days, and think if you really mean what you say or is it only a reverberation of things you have read, heard or seen.\n\nAnd even if it is the latter, have you critically thought about it, about all its facets, nuances, repercussions, etc. Could you be held responsible for what you say? Because somehow, the responsibility of saying what is right has drastically diminished. And especially when we say or write things on social media. The ethical onus of being right is no longer felt by most of us.\nBut the problem has graver consequences than you think, the homogeneity of untested ideas, of wants and aspirations. Let me elaborate on each of these.\nHype around generative AI, have we thought well about it? Hype about data science, how many companies are actually finding data science useful. Most of them are investing in the promise of insights from data. But are these insights valuable for all kinds of businesses? A more specific example, how much of machine learning which is being tried out in the medical domain will see the light of day when they have to go through all the regulations that the tools in the medical domain are usually subjected to. Even if they start it, it will take a decade to go through them.\nMany tech CEOs/CTOs/C-suite which are responsible for taking key decisions are not generally experts in AI but they are experts in running the business. As such, and rightly so, many of them rely on AI experts. But most strategies drown in the sea of self-proclaimed AI experts. Difference between hearsay and reliable references is very hard to tell. If you watch interviews of someone from the C-suite, you can clearly spot what terribly incorrect ideas of certain technologies make their way into the decision making process.\nThere is a phrase, ‚Äòshite talker‚Äô, which I have gotten to know recently and honestly, I am beginning to be fond of. Because it encapsulates the pride and confidence one feels, when one is asked to present in a conference, to give a talk, or write something for publishing online or even when asked for advice. Somehow we get so elated when we get these opportunities that we end up blabbering ideas, exuding confidence, in things we haven‚Äôt given much thought about. That is why I think we should always take someone speaking at a podium or a ‚Äòfireside chat‚Äô with a pinch of salt. Very few of us have been able to overcome this demon of herofying ourselves when we get these chances to express ourselves to a broader audience, who in our mind at least, hold us in high regard.\nLet me share an anecdote here.\nI was once attending a talk of a renowned professor in Dublin, he was from Stanford and quite a few big names from Dublin‚Äôs AI community had gathered to hear him. He was presenting his research and discussing the \\(r^2\\) metric for his models. I had read in a blog which had cited a paper (which made it very convincing to me) that \\(r^2\\) is not a good metric for non-linear models, so I asked him that question. And the room went awfully quiet for a second and he replied, astonished by the stupidity of the question asked, that it was just not true. My cover had been blown, and the rookie me had been exposed and I am sure some of you can relate to the humiliation of asking a stupid question in front of your peers. That evening I tugged my tail between my legs and went home.\nAfter reaching home I went through the blog again and also through the paper it cited. The paper referenced yet another paper and that paper cited another one. As such, going through a string of five papers over the week, I finally found the ground truth (pun intended üòâ). That paper listed different formulae for calculating \\(r^2\\) and explained the shortcomings of each of them. But it also offered a solution, a formula that doesn‚Äôt suffer from that problem. Somehow all those who based their work on this paper missed the key point of the paper and some of them would definitely have been peer reviewed.\nThis gives me serious trust issues, even more, some of you might be aware of it too, when Nobel worthy research is being discredited because of allegations against the researchers for concocting experimental results. Given these trust issues whenever I am looking for technical information on the web I usually look only at .edu sites from reputed institutes, but I am not convinced that the strategy would hold for long.\nOn a related note, what truly disappoints me is the news, it seems like every news channel has a set template which they try to fit to every story rather than tackling the nuances of things. These templates are ‚Äì rags to riches story, the rich or powerful not caring about the poor, scientific breakthroughs accompanied by women in science, etc. and you see these popping everywhere, news losing its very meaning, rude debates, personal biases of news readers and hosts, etc.\nSomehow I think that there is a part of us that trusts the information in books, in the news, on the internet, on blogs more than it deserves.\nOn the homogeneity of wants and aspirations‚Ä¶\nWe live in a consumerist world where we have somehow ended up producing much more we can consume. Corporations can pay exuberant salaries to MBAs, marketing and sales departments just to get rid of this surplus. Instead of writing more of these hackneyed facts here are some of my thoughts with no particular structure.\n\nWhen Taylor Swift sang Marry me, Juliet, you‚Äôll never have to be alone. Does she really mean it? How many married couples do actually feel they are never alone.\n\n\nDo you need the latest iphone and carry that heavy phone in your pocket? You buy more shite, which in turn propels more people even further to want those things.\n\n\nHow much of your wants and likings are your own and how much have you been conditioned to want? Difficult, might take you a lifetime to figure out.\n\nI have no clear remedy for the issue of taking limited responsibility that I have described here, but an example of a solution.\nOne of my professors back when I was doing my undergrad, Dr.¬†Bikram Phookun is one person that comes to my mind when I imagine someone who has really thought through and internalized what one is saying. Not only were his lectures a delight but otherwise as well he is quite an interesting person to talk to because, it seems to me, he has thought very deeply even about things we take for granted and has in turn given them a flavor of his own understanding. I always make it a point to read his social media posts no matter how lengthy they are or what they are about. My only regret is that he doesn‚Äôt post very often.\nBut maybe that is the price we pay for taking responsibility, we won‚Äôt have much to say. But what we say would be worth listening and trusting. And, if this article with a lot of scattered thoughts renders you mute and anxious about saying anything at least for a day I would consider a job well done."
  },
  {
    "objectID": "blog/training_on_edge.html",
    "href": "blog/training_on_edge.html",
    "title": "Training DNNs on Microcontrollers",
    "section": "",
    "text": "This post highlights a significant paper that came out in 2023 (and the best one I read in that year) which had a major impact on applied Deep Learning (DL). Although, the center of attention was undoubtedly the release of Chat-GPT and the accompanying frenzy around Large Language Models (LLMs), this piece of research would have lasting consequences for the few years to come.\nThe paper is on how we can train Deep Neural Networks (DNNs) on small devices like microcontrollers. Some of you working in DL might know that the most commonly used DNNs typically take a few hundred megabytes of memory, LLMs even more, around a few gigabytes. This is when there are billions of sensors, IoT devices, etc. where DNNs could be useful but cannot be deployed because these devices are small and typically have memories in the orders of a few megabytes, so the gap to fill is a few order of magnitudes large. Some work has been done previously on inference on these small devices, like, Tensorflow Lite. But there wasn‚Äôt much talk about training DNNs on these devices. And why so?\nObviously inference itself is too difficult and when you talk of training, backward propagation, the most popular way to train DNNs is a mammoth on memory, the backward graphs, the storing of intermediate gradients, etc. can take at least as much and in most cases much more memory than the forward pass. In fact, training on these small devices would about double the memory load, compounded by the fact that in the first place you squeezed in a model for inference, so there is even lesser memory left for backward propagation. So, the budget is very tight making the problem very complex. And this is precisely the problem this paper solved.\nOn-Device Training Under 256KB Memory  1, by Han lab from MIT made this significant breakthrough. I have been following Assoc. Prof.¬†Song Han for a while now, ever since his groundbreaking PhD thesis 2 in Tiny ML/Edge AI and would highly recommend reading this paper. As it is not just important but also a classic example of how complex engineering problems are solved‚Äì where you have a few innovative ideas which bundled together give you a rich solution and you can‚Äôt help admiring the creativity that went into it.\nThis paper combines three such ideas which are briefly described below."
  },
  {
    "objectID": "blog/training_on_edge.html#scaling-of-gradients",
    "href": "blog/training_on_edge.html#scaling-of-gradients",
    "title": "Training DNNs on Microcontrollers",
    "section": "1. Scaling of gradients",
    "text": "1. Scaling of gradients\nFor compression of DNNs quantization is widely used. As an oversimplified example, consider weights of a DNN which are usually of the 32 bit float format which as the name suggests takes 32 bits. Quantization would mean representing the same weights with lesser bits, let‚Äôs take 8 bits for our example. Now casting this 32 bit number to a 8 bit number (which can be fixed point or floating) takes scaling. One way to do it is to take the maximum of all weights in a feedforward layer or a convolutional layer and divide all the weights by it. The resulting weights would be between 0 and 1 and you can cast them to 8 bit format by multiplying with 127 (the 8 bit integer range is from -128 to 127). The authors had a very clever observation with this scaling, that it distorts the gradients.\n\n\n\n\n\nFig. 1: Distortion of gradients while quantizing 3\n\n\n\n\nConcentrate on the green and blue lines in the graph above. The blue line represents the ratio of the norm of weights and gradients while training a DNN in 32 bits and the green line represents the same for the quantized version of training. And one can see clearly how the ratios for the latter are much higher meaning that the gradients are much smaller as compared to weights for a quantized graph. This mismatch between the two lines shows how quantizing can distort the gradient-weight ratio and hence the training process, barring training convergence to a higher accuracy.\nAnd the solution proposed is, Quantization Aware Scaling (QAS) where they reduce this ratio mismatch by compensating the gradient by the square of the scaling factor. The QAS is shown by the yellow line in the graph and it closely matches the blue line for the unquantized 32 bits DNN.\nOne needs to think why the gradient of the quantized network has to be ‚Äòcompensated by the square of the scaling factor‚Äô. There is a way to understand that intuitively.\nConsider the graph below of the sigmoid function \\(f(x)= \\frac{1}{1+e^{-x}}\\). If we scale x by a factor of k, i.e.¬†\\(f(kx) = \\frac{1}{1+e^{-kx}}\\) the function distorts as shown in the figure. Usually, this is what one mathematically does when one wants to scale the function on the x-axis.\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAt the same time the gradient (or more popularly derivative) of the function is also distorted as shown in the figure below. But the derivative of \\(f(x)\\) distorts a little differently as compared to function \\(f(dx)\\).\n\n\n                            \n                                            \n\n\nLet‚Äôs understand this mathematically. \\[ \\frac{df(x)}{dx} \\rightarrow derivative\\ of\\ original\\ function \\] \\[ \\frac{df(kx)}{d(x)} \\rightarrow derivative\\ of\\ scaled\\ function \\] When weights are updated during backpropagation, we use the formula below, taking x as the weight and f(x) as the output of the layer:\n\\[x' = \\eta \\frac{df(x)}{dx} + x\\] where, \\(x' \\rightarrow\\) updated weight, \\(\\eta \\rightarrow\\) learning rate, \\(x \\rightarrow\\) old weight, and \\(\\frac{df(x)}{dx} \\rightarrow\\) derivative w.r.t output.\nFor a quantized version this formula should look like: \\[x' = \\eta \\frac{df(kx)}{dx} + kx\\]\nBut in reality when we backpropagate we calculate the derivative w.r.t kx instead of x and in practice the equation becomes \\[ x' = \\eta \\frac{df(kx)}{d(kx)} + kx \\] Applying chain rule: \\[ \\frac{df(kx)}{dx} = \\frac{df(kx)}{d(kx)} \\frac{d(kx)}{dx} \\] Where, \\(\\frac{d(kx)}{dx} = k\\) and substituting it back to the equation \\[ \\frac{df(kx)}{dx} = k \\frac{df(kx)}{d(kx)} \\] Making the weight update rule- \\[x' = \\eta \\frac{1}{k} \\frac{df(kx)}{dx} + kx\\]\nThis extra factor of \\(\\frac{1}{k}\\) is what we need to compensate. Authors also argue that as x is scaled by k so the gradient needs to have an extra factor of k. Hence, we multiply the gradient by \\(k^2\\) compensating it and thus preserving the ratios during training.\nI‚Äôll rather quickly go over the other two tricks that the paper proposes. After all, need to encourage you to read it."
  },
  {
    "objectID": "blog/training_on_edge.html#sparse-updates",
    "href": "blog/training_on_edge.html#sparse-updates",
    "title": "Training DNNs on Microcontrollers",
    "section": "2. Sparse updates",
    "text": "2. Sparse updates\nOn edge devices, we might not need to update the whole network; rather we can update only some layers whose weights and biases impact accuracy the most. They propose doing a contribution analysis of weights and biases of each layer and choosing to update only a selected few. More trends like bias update being cheap are also discussed in the paper. Sparse updates allow the backward graph of a DNN to be pruned thereby decreasing the memory footprint for training the DNN."
  },
  {
    "objectID": "blog/training_on_edge.html#embedded-mastery",
    "href": "blog/training_on_edge.html#embedded-mastery",
    "title": "Training DNNs on Microcontrollers",
    "section": "3. Embedded mastery",
    "text": "3. Embedded mastery\nLastly, some clever tricks in embedded programming are used in what the authors of the paper introduce as Tiny Training Engine (TTE). It creates a backward graph at compile time rather than runtime as pytorch or tensorflow. Dead code elimination, reordering operations so some can be fused together or used immediately instead of keeping them in memory, etc. are the cherry on the cake.\nAll this come harmoniously together enabling training DNNs on microcontrollers. Hope this brief overview kindled your interest in this line of research and if yes, do read their paper."
  },
  {
    "objectID": "blog/training_on_edge.html#footnotes",
    "href": "blog/training_on_edge.html#footnotes",
    "title": "Training DNNs on Microcontrollers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/mit-han-lab/tiny-training‚Ü©Ô∏é\nhttps://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented.pdf‚Ü©Ô∏é\nhttps://tinyml.mit.edu/on-device-training/assets/on-device-training.pdf‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/hiring_and_managing_data_scientists.html",
    "href": "blog/hiring_and_managing_data_scientists.html",
    "title": "Hiring and Managing Data Scientists",
    "section": "",
    "text": "This short post describes some learnings from my experience in Data Science and might give some useful tips for hiring and managing data scientists.\n\nAbout a year ago, I was looking for a new data science role. I had been working as a data scientist in an R&D centre of a university for more than three years and it was time for me to venture into the corporate world. And as with the usual job search I interviewed at a bunch of companies, and most of the interviews went well. Though I made some mistakes in a couple, but one of the interviews with a very well known company went horribly, and it discouraged me a lot as these were the early days of job hunting.\nDoubt about the usefulness of the skills I had learnt working at the university engulfed me and it felt I had to reskill a lot to move into the corporate. But as the overwhelming riptides of that hunt have ceded and having had the time to see for myself over the past year in my current role in the corporate, the intricacies of the data science role in corporations, I have realised that the problem was something else and goes much deeper. And it is about how companies see data scientists.\n\nIn the absence of well defined responsibilities, many companies over expect the skills of a data scientist.\n\nIf you are managing a team of data scientists, you would expect them to create models/algorithms with reasonable performance guarantees that can help your business. Or if it is about getting insights from data, some reliable, interpretable information in the form of reports, graphs, etc. But companies have gone a step ahead, sorry my bad, many steps ahead.\nMany companies, if you are a data scientist, expect you to not only create state-of-the-art models/algorithms but also expect an efficient and well tested code to implement it. It doesn‚Äôt end here, it also expects you to handle the data pipelines (which by the way is a data engineer‚Äôs job), databases and also make sure that you use the latest tools for deployment, like Docker and also configure those CI/CD pipelines. Woah! The list gets even bigger when data science projects are managed with Scrum.\nWell, I am not advocating that a data scientist should write poor code or be completely unaware of model deployment. But at the same time there is a difference between a data scientist, a data engineer and a software engineer. You need to ask yourself:\n\nWould you ask your accountant to be a lawyer just because they know tax laws?\n\nLet me elaborate, coding is one of the main tools used by data scientists, as such they pick a lot of skills that traditional coders pick like writing clean code, testing it, software design, etc. But at the same time you cannot expect your data scientist to be as proficient in these as a coder. The trend most data science teams are following is making the data scientist do all the jobs that otherwise would fit a software engineer.\nIf you want your data science team to tackle the whole pipeline of creating a ‚Äòdeployable‚Äô solution, then make sure you equip them with the necessary skill set either by training them or by getting expert software developers and data engineers helping your team. Though I am not personally in favour of the former, i.e., training data scientists to do everything, because there is always a time constraint involved, you always have a fixed time to do everything. And if you offload deployment to your data scientists you end up spending time that would have gone to model development to model deployment, the result being an inferior deployed model.\nThere is another problem with expecting your data scientists to excel at these skills‚Äì\n\nIt is just too damn hard for one person to master so many technical skills.\n\nYou already knew it, but somehow we forget this when we are managing people and have a tendency to make the most out of what we spend on our team. To circle back to that horrible interview, I was being asked about software design patterns, testing frameworks, other coding related and not so much about Machine Learning models. I felt being interviewed for a software engineering job and expressed my concerns to the interviewers. What came after is the second problem with the data science role.\nThe industry still seems to be experimenting with the ideal data science candidate. Data Scientist cadre comes from Statistics, Biology, Physics, Mathematics, Computer Science, and some other eccentric degrees. Masters and bachelors courses in data science are new and will take time to gain traction. As such, even when talking about core data science skills the corpus to go through is extremely large and varied. And no one would disagree that once a data scientist starts working on a project, the knowledge of that domain is paramount to the success of that project.\n\nDomain specific data scientists might be the way ahead.\n\nHence, some companies, have rather smartly, started specialising the data science roles. You will see a ‚Äòmarketing data scientist‚Äô role advertised in Dublin quite often. Which I feel is a good start, rather than hiring a data scientist with financial services experience to do marketing related projects. And this is the trend I see ahead and some of you might also find it useful when hiring data scientists to look for domain specific data scientists, of course if it suits you. But many of you will be companies which are just starting off their data science journey and there won‚Äôt be many data scientists with that domain knowledge. In that case, use your sane judgement and probably do not hire someone way off, like, a data scientist working for a bank on mainly tabular data may not be the best if you want to do computer vision tasks.\nThis simple sense is not very commonly seen. So, let us delve deeper into it. There are literally thousands of research papers out there with models/algorithms, and some of these have been nicely coded into Python libraries like Sklearn, numpy, pytorch, keras, tensorflow, etc. Contrary to what many think, data science is not about using these off the shelf Python libraries and solve the problem, it still involves a lot of research and innovation, a data scientist has to dwell into the problem, make amendments to these to suit their needs and that there is the applied research bit which is still very prevalent in data science problems.\nTo give an example, coming back to that interview, we were talking about hypothesis testing, obviously because the interviewer had a PhD in statistics. Honestly, I was a bit rusty about it and he asked, ‚ÄòHow do you compare your machine learning models if you don‚Äôt know about a so and so test‚Äô. There was an awkward pause because that sounded new, even to the other interviewer. But I went ahead and explained about different metrics that can be used like f1-score, etc. But he soon came back to hypothesis testing.\nAnd here is the problem, hypothesis testing is not very useful for machine learning models because it is quite expensive to compute. For most tests you would want to run your models/algorithms hundreds of times to get a test statistic which has a high enough power. Hence, you seldom see hypothesis testing being used in machine learning literature to compare models. During the two interviews which I had with that company, both of them at least an hour long, that was the only question related to models/algorithms that I was asked. As I said earlier, the recommended corpus is too large for data science and this situation was just a symptom of that problem. An alternative and in all ways a better way to interview, if you are interviewing an experienced candidate, would be to ask the candidate about their previous projects, delve deeper into how they chose models, what metrics did they use, did they design new algorithms, etc. And that was precisely my job interview at the company I am currently working for and many others for which I interviewed.\n\nTo weave these threads together here are the two key takeaways if you are managing or hiring data scientists ‚Äî\n\nData science roles are already very generic, let‚Äôs not try to make it even more generic by expecting a data scientist to be a software engineer.\nWhen hiring, value domain expertise or at least the ability of the person being hired to pick up the domain knowledge.\n\n\nHope that gives you some perspective and if you have any suggestions or comments feel free to mention them below."
  },
  {
    "objectID": "portfolio/AI4EO.html",
    "href": "portfolio/AI4EO.html",
    "title": "AI for Earth Observation",
    "section": "",
    "text": "Summary\nDeveloped standards and specifications for European Space Agency and worked on image segmentation for EO data.\n\n\nThe story\nEarth Observation (EO) data, comprising of data coming from satellites or drones presents vast opportunities for industry and AI. As such, we worked on AIREO with European Space Agency to formulate specifications and standards for EO data so that it can be easily used by the AI community.\nWe gathered surveys from the community and then specified standards for EO data including what metadata should be present and how we can make it easier to ingest EO data into tools commonly used by AI practitioners. Best practices on how to train ML algorithms on EO data were also laid down.\n\n\n\n\n\nFig. 1: Different facets of EO data covered by AIREO specifications\n\n\n\n\n\n\n\nImportant links\n\nLink to European Space Agency‚Äôs AIREO project\nhttps://eo4society.esa.int/projects/aireo/\n\n\nLink to CeADAR‚Äôs AI for Earth Observation demonstrator\nhttps://ceadar.ie/blog/ai-for-earth-observation/\n\n\nPublication\n\nAlastair McKinstry, Oisin Boydell, Quan Le, Inder Preet, Jennifer Hanafin, Manuel Fernandez, Adam Warde, Venkatesh Kannan, and Patrick Griffiths, ‚ÄúAI-Ready Training Datasets for Earth Observation: Enabling FAIR data principles for EO training data‚Äù. EGU General Assembly, 2021, link."
  },
  {
    "objectID": "portfolio/wind_energy.html",
    "href": "portfolio/wind_energy.html",
    "title": "Wind Energy Analytics",
    "section": "",
    "text": "Summary\nExplored support vector machines for hind casting wind speed data for wind farms and created an open-source library for wind energy analysis.\n\n\nThe story\nWhat started as my master‚Äôs thesis on wind resource assessment turned into first open source python library for wind energy analytics. Working at Brightwind Analysis we were looking at ways to predict the wind energy at a site using machine learning. It is necessary to predict it accurately for the next twenty years before setting up a wind farm to evaluate financial viability of setting up a farm. We ended up writing enough code to feel the need to open source it so other analysts can save time using it. It ended up being a popular project with many adopting the tool for their everyday wind energy assessment.\nThe documentation of the library can be found here.\nSome cool things the library can do:\n\n\n\nAdvanced plots for wind analysts\n\n\n\n\n\nFig. 1: Wind rose for turbulence calculation.\n\n\n\n\nAnalyzing different types of wind distribution\n\n\n\n\n\nFig. 3: Wind distribution analysis\n\n\n\n\n\n\n\nShear calculation\n\n\n\n\n\nFig. 2: Shear profile calculation\n\n\n\n\nA suite of wind speed forecasting methods, including ones proposed in my thesis\n\n\n\n\n\nFig. 4: Simple linear regression model for wind speed prediction\n\n\n\n\n\n\n\n\n\nLink to brightwind open-source python library\nhttps://github.com/brightwind-dev/brightwind"
  },
  {
    "objectID": "portfolio/telecom_data.html",
    "href": "portfolio/telecom_data.html",
    "title": "Telecom Data Analytics",
    "section": "",
    "text": "Summary\nWorked on telecom network data to identify anomalies and correlation between different KPIs to track the source of anomaly.\n\n\nThe story\nWorking with Sonalake state-of-the-art time-series analytics methods were used to solve various problems in telecom network management.\n\nThe first one was trend analysis, where the trends in some key KPIs could help plan a telecom network operator figure out where to install additional capacity and how to plan future expansion. We used decomposition methods to better predict trends in various KPIs.\n\n\n\n\n\nFig. 1: Decomposition of time series to better predict trends in KPIs\n\n\n\n\nAnomaly detection is another area where telecom networks need some automation, as there are literally hundreds of KPIs which they monitor and manually looking for anomalies is not feasible. Automated ways to detect anomalies can improve quality of service. We used several methods to detect anomalies automatically in each of their KPIs and flag them in a timely manner.\n\n\n\n\n\nFig. 2: Anomaly detection in a typical telecom KPI\n\n\n\n\nOnce the anomaly is identified, we also need to ascertain what services would it impact or what could have been the source of the anomaly. This is particularly difficult for telecom data as hundreds of KPIs are involved making their tracking manually, intractable. Hence, we used self organizing maps to automatically find relationships between the KPIs that might suggest which ones can be a probable cause of failure or could be impacted by an anomaly.\n\n\n\n\n\n\nFig. 3: Self organizing map for telecom KPIs\n\n\n\n\n\n\nBlog post from Sonalake‚Äôs website summarizing this work\nhttps://sonalake.com/latest/applying-analytics-and-data-science-in-telecoms-network-congestion-forecasting/"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Generative AI \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient/Edge AI \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Earth Observation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrivacy Preserving ML\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTelecom Data Analytics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Analytics\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]