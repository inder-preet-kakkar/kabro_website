---
title: <b>NeurIPS 2025 Takeaways</b>
order: 5
from: markdown+emoji
date: 11 December, 2025
Words: ~1000
---

## Benchmarks for agents and beyond

The very first day was quite interesting with a lot of contrasting ideas. The very first talk was from Apple, where they showed how reasoning models behave when games get complex. They discovered something counter-intuitive: at low-complexity games, non-reasoning models behave the same as reasoning models; at medium complexity, reasoning models are better; but at high complexity, both are bad—despite increasing the thinking budget. And, quite interestingly, the number of thinking tokens also does not increase indefinitely; it plateaus once complexity is high enough.

The same day there was an extensive tutorial on benchmarks, including a few interesting takes from the public-policy angle, Meter the startup, coding benchmarks, and so on.

1. It is tricky to talk about creating a benchmark. Some have stood the test of time—or, as someone said, are being used far beyond where they *should* be.
2. One of the speakers from Meter also expressed some displeasure about ‘exotic’ benchmarks like ARC-AGI, which he claimed (I’m paraphrasing) do not correspond to real-world applications and are instead someone’s idea of what intelligence *could* be—spatial reasoning in this case. I have often, as might many of you, been annoyed preparing for these exams that give you riddles testing what they call spatio-temporal **reasoning**. Is that real intelligence? Is that even a good proxy?

That inevitably made me think of the Apple talk in the morning, which now also seemed like an ‘exotic’ game chosen as a proxy for intelligence. The authors were aware of this and have responded to critics, but the justification felt less convincing when viewed from this angle. The strategy seems to be: choose a game the LLMs have likely never seen during training and then extrapolate performance there to draw conclusions about general reasoning capabilities. But yes, I do understand that intelligence itself is an ill-defined concept and this evaluation problem will always be slippery. Nevertheless, these studies and ‘exotic datasets’ are interesting because they shed light on new domains for LLMs—and who knows where the next breakthrough comes from.

Apart from these highly philosophical discussions, there were some very practical trends around benchmarking agents. We are all aware of the limitations of traditional metrics—the overly homogeneous test sets, data drift, and so on. But a new, dynamic testing framework is emerging with agents: generating new test instances during evaluation, keeping the test set alive instead of static. The key metric becomes whether the agent accomplishes the task, however you want to quantify that, instead of the accuracy of components like word-error rate or F1 score. A clear shift from what I’d call *academic* metrics to more product-centric ones—nobody should be unhappy about that.

One of the best examples came at the end of the day from Tesla’s driving agent. They built a world model from the humongous driving data they have, and they can edit the inputs to this world model using natural language to create new situations. A closed-loop simulator for testing *and* training agents. Honestly the best production-grade example I saw.

BeeAI framework from IBM also fits neatly into this new paradigm of dynamic evaluation, though I’ll write more on that separately.


## Coding agents

Research and industry both recognize the limitations of current coding agents when it comes to production-quality code. Everyone acknowledges the issues with SWE-bench, and they admit that getting real production data for coding benchmarks is still a major bottleneck. If that’s the bottleneck, then are we just one step away from Microsoft (owning GitHub) or Google—with their armies of engineers—locking down the advantage? Maybe. But Claude Code doing extremely well keeps me hopeful that we’ll see meaningful competition rather than consolidation.

A huge proportion of papers on coding agents revolve around clever prompting strategies and decomposing problems into flows that *might* help the agent. But isn’t that just what all of us do anyway? You sketch a high-level plan before solving anything. It reminds me of a few years ago when ML use exploded and thousands of papers emerged where people applied standard ML techniques to a new domain, and master’s theses consisted of doing a giant grid search. Mine was the same, by the way ;)

A cute piece of work was [Paper2Code](https://github.com/going-doer/Paper2Code)—check out their repo—evaluating code generation directly from research papers. Something we’ve all desperately needed, so kudos to them. A few others worth mentioning were MOSAIC and ChopChop.

IBM’s ALICE—Agentic Logic for Incident and Code bug Elimination—is another one I’m proud to see progress. ALICE brings together multi-agent collaboration, incident analysis, and code-level reasoning to help teams diagnose complex system issues faster and more intelligently. This sits right at the intersection of AI agents, observability, and software automation—an area that’s evolving ridiculously fast, and I’m grateful to be contributing to it.



## Efficient AI

There was a lot happening on Efficient AI. The most intriguing work was from companies like [Eigen AI](https://www.eigenai.com/), [Pruna AI](https://www.pruna.ai/), [Furiosa AI](https://furiosa.ai/), and [Qualcomm](https://www.qualcomm.com/)—check out their demo on disaggregated LLM serving on AI accelerators. A lot of interesting architectural directions are emerging.

One thread that stood out: **Jet-Nemotron** and **radial attention**—a sparse attention mechanism for long video generation. Sparse models are clearly becoming the theme again, but this time in a more structured, hardware-aware way.


## Wrapping up

As always, a lot of the best parts of NeurIPS happen outside the sessions. The side conversations were rich—people comparing notes on agent evaluations, debugging LLM behaviors in the wild, trying to make sense of the rapid shifts in hardware, and the shared feeling that we’re building tools faster than we can define what “good” looks like. It’s chaotic, but in the fun way.

Overall, NeurIPS 2025 felt like a pivot year: agents becoming the dominant framing, evaluation moving from static to dynamic, efficiency research finally aligning with real hardware, and coding agents inching toward systems-level usefulness.

And yes—I left with more questions than answers, but that’s the whole point of going.

